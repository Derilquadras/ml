{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 : Artificial Neural Network, Backpropagation and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "    each sample in dataset has \n",
    "'''\n",
    "        # [ hours_of_sleeping, hours_of_studying,  marks_obtained_in_a_test ]          \n",
    "\n",
    "\n",
    "dataset = [  [2, 9, 92],\n",
    "             [1, 5, 86],   \n",
    "             [3, 6, 89]\n",
    "          ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ### Preparing inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forming inputs and outputs for the Neural Network\n",
    "\n",
    "inputs  = [ sample[:-1]      for sample in dataset ]\n",
    "outputs = [ [ sample[-1] ]   for sample in dataset ]\n",
    "\n",
    "            # Notice : extra big brackets for generating column array, highly critical if it is ignored \n",
    "\n",
    "\n",
    "# converting normal arrays to numpy float arrays,\n",
    "\n",
    "inputs = np.array(inputs, dtype=float)\n",
    "outputs =  np.array(outputs, dtype=float)           \n",
    "\n",
    "\n",
    "# Normalizing by max divide, The inputs and outputs shoud be in range [0, 1]\n",
    "\n",
    "inputs = inputs/np.amax(inputs, axis=0)     # input has 2 columns, dividing each column with maximum of respective column\n",
    "outputs = outputs/100                       # since max test score is 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation function \n",
    "\n",
    "def sigmoid(s):\n",
    "    \n",
    "    return 1/(1 + np.exp( -s ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dervative of activation function which is necessary for backtracking \n",
    "\n",
    "def sigmoid_derivative(s):\n",
    "    \n",
    "        return s * (1 - s)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## static single hidden layered Neural Net class without biases\n",
    "<img src=\"neuralnet.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    \n",
    "    \n",
    "    def __init__(self, inputs, outputs):\n",
    "        \n",
    "                   # Initialization\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        \n",
    "        #--------------------------------------------------------\n",
    "        \n",
    "                   # Parameters\n",
    "        self.input_size  = len(inputs[0])\n",
    "        self.output_size = 1\n",
    "        self.hidden_size = 3                   # number of neurons in hidden layer\n",
    "    \n",
    "        #--------------------------------------------------------\n",
    "\n",
    "                                               # random weights for first layer and second layer'\n",
    "            \n",
    "            \n",
    "        self.weights1 = np.random.randn(self.input_size, self.hidden_size)        # (3x2) weight matrix from input to hidden layer\n",
    "        self.weights2 = np.random.randn(self.hidden_size, self.output_size)       # (3x1) weight matrix from hidden to output layer\n",
    "\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        ''' forward propogation  '''\n",
    "        \n",
    "\n",
    "    #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++# \n",
    "    \n",
    "        self.z1 = np.dot(inputs, self.weights1)        # dot product of input layer and first set of 3x2 weights\n",
    "        self.z2 = sigmoid(self.z1)                     # activation function applied on previous output\n",
    "     \n",
    "    #+-------------------------------------------------------------------------------------------------------+#                          \n",
    "\n",
    "        self.z3 = np.dot(self.z2, self.weights2)       # dot product of hidden layer and second set of 3x1 weights\n",
    "        obtained_output = sigmoid(self.z3)             # final activation function\n",
    "\n",
    "    #+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++#                          \n",
    "\n",
    "        return obtained_output \n",
    "\n",
    "    \n",
    "    def backward(self, obtained_output):\n",
    "      \n",
    "        ''' backward propgate through the network    '''\n",
    "        \n",
    "        \n",
    "                # error in output\n",
    "        output_error = self.outputs - obtained_output \n",
    "              \n",
    "                # applying derivative of sigmoid to obtained outputs\n",
    "        output_delta = output_error * sigmoid_derivative(obtained_output) \n",
    "        \n",
    "        '''---------------------------------------------------------------'''\n",
    "\n",
    "                 # z2 error:  hidden layer weights contribribution to output error\n",
    "        z2_error = output_delta.dot( self.weights2.transpose()    )   \n",
    "              \n",
    "                 # applying derivative of sigmoid to z2 error\n",
    "        z2_delta = z2_error * sigmoid_derivative(self.z2) \n",
    "        \n",
    " \n",
    "        '''---------------------------------------------------------------'''\n",
    "    \n",
    "        #----------------------------------------------------#\n",
    "        #   updating weights, wj -> wj + n * delta(wj)       #\n",
    "        #                                n, learning rate    #\n",
    "        #----------------------------------------------------#\n",
    "        \n",
    "                  # adjusting first set (input --> hidden) weights\n",
    "        self.weights1 = self.weights1 +   (0.5 * self.inputs.T.dot(z2_delta) )     #.T stands for transpose\n",
    "        \n",
    "                  # adjusting second set (hidden --> output) weights\n",
    "        self.weights2 = self.weights2 +   (0.5 * self.z2.T.dot(output_delta) )  \n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        ''' training  the network'''\n",
    "        \n",
    "        obtained_output = self.forward(self.inputs)\n",
    "        self.backward(obtained_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Creating a neural net and training it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch->  0  Loss: 0.07274521673240818\n",
      "Epoch->  1  Loss: 0.0585782542249422\n",
      "Epoch->  2  Loss: 0.047897611723800394\n",
      "Epoch->  3  Loss: 0.039725730089891075\n",
      "Epoch->  4  Loss: 0.03337521083036012\n",
      "Epoch->  5  Loss: 0.028364553631714123\n",
      "Epoch->  6  Loss: 0.0243542932035502\n",
      "Epoch->  7  Loss: 0.021102324867346034\n",
      "Epoch->  8  Loss: 0.018433588767724624\n",
      "Epoch->  9  Loss: 0.01621966047452225\n",
      "Epoch->  10  Loss: 0.014364967246628403\n",
      "Epoch->  11  Loss: 0.01279739659922404\n",
      "Epoch->  12  Loss: 0.011461822793107186\n",
      "Epoch->  13  Loss: 0.01031558692703971\n",
      "Epoch->  14  Loss: 0.009325299066303581\n",
      "Epoch->  15  Loss: 0.008464545890300382\n",
      "Epoch->  16  Loss: 0.007712226436721825\n",
      "Epoch->  17  Loss: 0.007051329051034897\n",
      "Epoch->  18  Loss: 0.006468022117193933\n",
      "Epoch->  19  Loss: 0.005950970628860717\n"
     ]
    }
   ],
   "source": [
    "net = Neural_Network(inputs, outputs)\n",
    "\n",
    "\n",
    "for i in range(20):                   #  the more you train, the  less error you obtain untill it overfits \n",
    "    \n",
    "    loss =  np.mean( np.square(outputs - net.forward(inputs)))      # mean sum squared loss\n",
    "    print (\"Epoch-> \", i, \" Loss:\", loss)     \n",
    "    net.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      "\n",
      "[[1.         0.55555556]\n",
      " [0.5        1.        ]\n",
      " [1.         0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "test = np.array( [ [2, 5], [1, 9], [2, 3] ], dtype=float)    # same as preparing inputs\n",
    "test = test/np.amax(test, axis=0)\n",
    "\n",
    "print(\"Input: \",test, sep='\\n\\n')                            # Normalized input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Output\n",
      "\n",
      "[[86.19906654]\n",
      " [79.93228323]\n",
      " [86.8421938 ]]\n"
     ]
    }
   ],
   "source": [
    "output_of_test =  net.forward(test) * 100\n",
    "\n",
    "print(\"Predicted Output\", output_of_test, sep='\\n\\n')        # predicted marks for each sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> div.prompt_container{ width: 0px; min-width: 0px; visibility: collapse}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style> div.prompt{ width: 0px; min-width: 0px; visibility: collapse}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style> div.output pre { font-style: italic; color: rgb(105,105,105) } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
