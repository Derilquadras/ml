# Machine Learning Lab Programs 15CSL76 ( VTU )

Unlike any other repositories, this has ML lab programs,
But this repo is aimed at providing readable and succinct notebooks and will be maintained for decades to come.

Most of the notebooks are being copied from <a href=https://github.com/profthyagu> @profthyagu</a>'s repositories and made pythonic to a certain extent.

<br>
<br>
<i>Below programs are hyperlinked to colab notebooks, so that you can inspect them on the go. </i> 

## All programs 
- **Find_S**  <br>
Implement and demonstrate the FIND-S algorithm for finding the most specific hypothesis based on a given set of training data samples. Read the training data from a .csv file.<pre>  <a href="https://github.com/ravish0007/ml/blob/master/pdfs/1_find_s.pdf"> <img src="https://image.flaticon.com/icons/png/128/179/179483.png" height=50px width=50px align = "center"> </a>        <a href="https://colab.research.google.com/gist/ravish0007/faf21ae1c6d436630d87d2b92d702e66/1_find_s.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" align="center"></a></pre> 


- **Candidate_Elimination** <br>
For a given set of training data examples stored in a .CSV file, implement and demonstrate the Candidate-Elimination algorithm to output a description of the set of all hypotheses consistent with the training examples.<pre>  <a href="https://github.com/ravish0007/ml/blob/master/pdfs/2_candidate_elimination.pdf"> <img src="https://image.flaticon.com/icons/png/128/179/179483.png" height=50px width=50px align = "center"> </a>        <a href="https://colab.research.google.com/gist/ravish0007/ca03476e8a0054a9025d9acef518c935/2_candidate_elimination.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" align="center"></a>  </pre>

- **Decision_Tree_ID3** <br>
Write a program to demonstrate the working of the decision tree based ID3 algorithm. Use an appropriate data set for building the decision tree and apply this knowledge to classify a new sample.<pre>  <a href="https://github.com/ravish0007/ml/blob/master/pdfs/3_decision_tree_ID3.pdf"> <img src="https://image.flaticon.com/icons/png/128/179/179483.png" height=50px width=50px align = "center"> </a>        <a href="https://colab.research.google.com/gist/ravish0007/c3ae56f4e62b70b03d77362081b912f8/3_decision_tree_id3.ipynb" > <img src="https://colab.research.google.com/assets/colab-badge.svg" align="center"></a> </pre>

- **ANN_Backtracking** <br>
Build an Artificial Neural Network by implementing the Backpropagation algorithm and test the same using appropriate data sets.<pre>  <a href="https://github.com/ravish0007/ml/blob/master/pdfs/4_ann_backprop.pdf"> <img src="https://image.flaticon.com/icons/png/128/179/179483.png" height=50px width=50px align = "center"> </a>        <a href="https://colab.research.google.com/gist/ravish0007/3ca160a9c4c58630a235d1148bfaa210/4_ann_backprop.ipynb" > <img src="https://colab.research.google.com/assets/colab-badge.svg" align="center"></a> </pre>


- **Naïve_Bayes_Classifier** <br>
Write a program to implement the naïve Bayesian classifier for a sample training data set stored as a .csv file. Compute the accuracy of the classifier, considering few test data sets.<pre>  <a href="https://github.com/ravish0007/ml/blob/master/pdfs/5_naive_bayes_classifier.pdf"> <img src="https://image.flaticon.com/icons/png/128/179/179483.png" height=50px width=50px align = "center"> </a>        <a href="https://colab.research.google.com/gist/ravish0007/7af2297f005a81391653b36372fc9556/5_naive_bayes_classifier.ipynb" > <img src="https://colab.research.google.com/assets/colab-badge.svg" align="center"></a> </pre>


- **Naïve_Bayes_Document_Classifier**<br>
Assuming a set of documents that need to be classified, use the naïve Bayesian Classifier model to perform this task. Built-in Java classes/API can be used to write the program. Calculate the accuracy, precision, and recall for your data set.<pre>  <a href="https://github.com/ravish0007/ml/blob/master/pdfs/6_naive_bayes_doc_classifier.pdf"> <img src="https://image.flaticon.com/icons/png/128/179/179483.png" height=50px width=50px align = "center"> </a>        <a href="https://colab.research.google.com/gist/ravish0007/7642d5678c1a842ac00e8da442f62029/6_naive_bayes_doc_classification.ipynb" > <img src="https://colab.research.google.com/assets/colab-badge.svg" align="center"></a> </pre>

- **Bayesian_Network**<br>
Write a program to construct a Bayesian network considering medical data. Use this model to demonstrate the diagnosis of heart patients using standard Heart Disease Data Set. You can use Java/Python ML library classes/API.<pre>  <a href="https://github.com/ravish0007/ml/blob/master/pdfs/7_bayesian_network.pdf"> <img src="https://image.flaticon.com/icons/png/128/179/179483.png" height=50px width=50px align = "center"> </a>        <a href="https://colab.research.google.com/gist/ravish0007/2229576288831102137adb4e0a1f2c91/7_bayesian_network.ipynb" > <img src="https://colab.research.google.com/assets/colab-badge.svg" align="center"></a> </pre>

- **KMeans_vs_EM** <br>
Apply EM algorithm to cluster a set of data stored in a .CSV file. Use the same data set for clustering using k-Means algorithm. Compare the results of these two algorithms and comment on the quality of clustering. You can add Java/Python ML library classes/API in the program.<pre>  <a href="https://github.com/ravish0007/ml/blob/master/pdfs/8_k_means_gmm.pdf" > <img src="https://image.flaticon.com/icons/png/128/179/179483.png" height=50px width=50px align = "center"></a>          <a href="https://colab.research.google.com/github/ravish0007/ml/blob/master/8_k_means_gmm/8_k_means_gmm.ipynb" ><img src="https://colab.research.google.com/assets/colab-badge.svg" align="center"></a></pre>

- **K_Nearest_Neighbour_Classifier**<br>
Write a program to implement k-Nearest Neighbour algorithm to classify the iris data set. Print both correct and wrong predictions. Java/Python ML library classes can be used for this problem.<pre>  <a href="https://github.com/ravish0007/ml/blob/master/pdfs/9_k_nearest_neighbour.pdf" > <img src="https://image.flaticon.com/icons/png/128/179/179483.png" height=50px width=50px align = "center"></a>          <a href="https://colab.research.google.com/github/ravish0007/ml/blob/master/9_k_nearest_neighbour/9_k_nearest_neighbour.ipynb" ><img src="https://colab.research.google.com/assets/colab-badge.svg" align="center"></a></pre>

- **Locally_Weighted_Regression** <br>
Implement the non-parametric Locally Weighted Regression algorithm in order to fit data points. Select appropriate data set for your experiment and draw graphs.<pre>  <a href="https://github.com/ravish0007/ml/blob/master/pdfs/10_local_weighted_regression.pdf" > <img src="https://image.flaticon.com/icons/png/128/179/179483.png" height=50px width=50px align = "center"></a>          <a href="https://colab.research.google.com/github/ravish0007/ml/blob/master/10_local_weighted_regression/10_local_weighted_regression.ipynb" ><img src="https://colab.research.google.com/assets/colab-badge.svg" align="center"></a></pre>

<br><br>
# Contributors

* [Ravish](https://github.com/ravish0007)<br>
* [Vishal](https://github.com/LastBencher-98)<br>


# References

* [Machine Learning by Thomas M. Mitchell, McGraw-Hill, Inc. New York, NY, USA ©1997](http://profsite.um.ac.ir/~monsefi/machine-learning/pdf/Machine-Learning-Tom-Mitchell.pdf)

# Further Readings
* [Stanford CS109](http://web.stanford.edu/class/cs109/)
* [ciml](http://ciml.info/)
* [Stanford CS229](http://cs229.stanford.edu/)
* [MIT 6.867](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/)
* [Stanford CS230](https://cs230.stanford.edu/)
* [Stanford CS224d](https://cs224d.stanford.edu/)
* [MIT 6.867 ](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-867-machine-learning-fall-2006/)
* [MIT 6.S191](http://introtodeeplearning.com/)
* [Introduction to Deep Learning by  Michael Nielsen](http://neuralnetworksanddeeplearning.com/)
* [Stanford CS231n](http://cs231n.github.io/)
* [Google Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course)
* [Ian Goodfellow's Book on deeplearning](https://www.deeplearningbook.org/)
* [Artificial Intelligence: A Modern Approach](http://aima.cs.berkeley.edu/)
* [MIT 6.034](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/)
* [Stanford CS221](https://stanford-cs221.github.io/autumn2019/)
* [fast.ai Deep Learning Course]( http://course.fast.ai)



